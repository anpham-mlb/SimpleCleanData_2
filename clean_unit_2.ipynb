{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "# Method\n",
    "The process performed to do the task 2:\n",
    "- Normalise tokens to lowercase except the capital tokens appeared in the middle of a sentence/line\n",
    "- Tokenize words by using the regular expression \"\\w+(?:[-']\\w+)?\"\n",
    "- Return a set of vocabulary with first 200 meaningfull bigrams included in\n",
    "- Remove the context-independent and context-dependent stopwords from the vocabulary\n",
    "- Remove rare tokens from the vocabulary\n",
    "- Stem tokens by using Porter stemmer\n",
    "- Remove tokens with the length less than 3 from the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import necessary modules for task 2 such as pandas and numpy and read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Title                                           Synopsis  \\\n",
       "0  FIT2100  This unit will provide students with the knowl...   \n",
       "1      NaN      how a multi-programming, multi-user operating   \n",
       "2      NaN       systems operates and it manages and allocate   \n",
       "3      NaN  resources to different applications. Students ...   \n",
       "4      NaN      able to compare and contrast various resource   \n",
       "\n",
       "                                            Outcomes  \n",
       "0  ['analyse and evaluate various strategies used...  \n",
       "1  operating system in managing the system resources  \n",
       "2  and running applications efficiently;', 'analy...  \n",
       "3  identify parameters that can improve the perfo...  \n",
       "4  of multi-programming operating systems;', 'app...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Synopsis</th>\n      <th>Outcomes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FIT2100</td>\n      <td>This unit will provide students with the knowl...</td>\n      <td>['analyse and evaluate various strategies used...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>how a multi-programming, multi-user operating</td>\n      <td>operating system in managing the system resources</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>systems operates and it manages and allocate</td>\n      <td>and running applications efficiently;', 'analy...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>resources to different applications. Students ...</td>\n      <td>identify parameters that can improve the perfo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>able to compare and contrast various resource</td>\n      <td>of multi-programming operating systems;', 'app...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('unit.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Title                                           Synopsis  \\\n",
       "0  ACB2851  The objective of this unit is two-fold. First,...   \n",
       "1  ACS2700  This unit provides an introduction to ethical ...   \n",
       "2  ACW3050  Topics covered by this unit include: Australia...   \n",
       "3  AHT3105  This unit considers contemporary international...   \n",
       "4  AMU1305  Film studies: Forms and approaches gives stude...   \n",
       "\n",
       "                                            Outcomes  \n",
       "0  ['examine the role of accounting information s...  \n",
       "1  ['examine the ethical dimension of individual ...  \n",
       "2  ['critically analyse how standard setting proc...  \n",
       "3  ['Identify the wider concerns of contemporary ...  \n",
       "4  ['Analyse films with particular attention to t...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Synopsis</th>\n      <th>Outcomes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ACB2851</td>\n      <td>The objective of this unit is two-fold. First,...</td>\n      <td>['examine the role of accounting information s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ACS2700</td>\n      <td>This unit provides an introduction to ethical ...</td>\n      <td>['examine the ethical dimension of individual ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ACW3050</td>\n      <td>Topics covered by this unit include: Australia...</td>\n      <td>['critically analyse how standard setting proc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AHT3105</td>\n      <td>This unit considers contemporary international...</td>\n      <td>['Identify the wider concerns of contemporary ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AMU1305</td>\n      <td>Film studies: Forms and approaches gives stude...</td>\n      <td>['Analyse films with particular attention to t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data.Title = data.Title.ffill()\n",
    "#Replace null values in Title axis by values before\n",
    "\n",
    "data_synopsis = data.groupby('Title')['Synopsis'].apply(lambda x: ' '.join([y for y in x if not pd.isnull(y)]))\n",
    "#Join rows of Synopsis column accroding to title and put the result into a table called data_synopsis\n",
    "\n",
    "data_outcomes = data.groupby('Title')['Outcomes'].apply(lambda x: ' '.join([y for y in x if not pd.isnull(y)]))\n",
    "#Join rows of Outcomes column accroding to title and put the result into a table called data_outcomes\n",
    "\n",
    "data_outcomes = data_outcomes.to_frame()\n",
    "data_synopsis = data_synopsis.to_frame()\n",
    "#Convert data_outcomes and data_synopsis into dataframe\n",
    "\n",
    "data = pd.merge(data_synopsis, data_outcomes, on = 'Title')\n",
    "#Merge two dataframe above into a dataframe\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "#Reformat the dataframe\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Remove unecessary character in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import re module to use regular expression\n",
    "import re\n",
    "\n",
    "#Create a function that returns any content from after ' or \" to before ' or \" and , or ]\n",
    "def clean_outcomes(text):\n",
    "    if pd.isnull(text):\n",
    "    #For rows are null, return an empty list\n",
    "        return []\n",
    "    r = r\"['\\\"](.*?)['\\\"](?=[,\\]])\"\n",
    "    #The pattern will match any content from after ' or \" to before ' or \" and , or ]\n",
    "    return re.findall(r, text)\n",
    "\n",
    "data.Outcomes = data.Outcomes.apply(lambda x: clean_outcomes(x))\n",
    "#Apply the function into the content of each unit in Outcomes column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Normalize tokens to lowercase\n",
    "- First, tokenize text into sentences\n",
    "- Second, normalize tokens to lowercase except the capital tokens appeared in the middle of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mikeloongboong/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['examine the role of accounting information systems in analysing and providing decision support to managers',\n",
       " 'explain the design of accounting information systems and financial models',\n",
       " 'develop financial models to assist in decision making',\n",
       " 'apply critical thinking, problem solving and presentation skills to individual and / or group activities dealing with accounting information systems and demonstrate in an individual summative assessment task the acquisition of a comprehensive understanding of the topics covered by ACB2851.']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#First, tokenize text into sentences\n",
    "#Install nltk, nltk data and import sent_tokenize tool from nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#Create a function to tokenize text into sentences\n",
    "def segmentation(text):\n",
    "    #For content are null, return an empty list. Otherwise, tokenizing content\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    return sent_detector.tokenize(text)\n",
    "\n",
    "# Apply tokenizing function into content of each unit of column Outcomes and Synopsis \n",
    "# The result will be a list of each sentence\n",
    "data.Outcomes = data.Outcomes.apply(lambda x: [z for y in x for z in segmentation(y)])\n",
    "data.Synopsis = data.Synopsis.apply(lambda x: segmentation(x))\n",
    "\n",
    "# Second, normalize first character of each sentence to lowercase except the capital tokens \n",
    "# appeared in the middle of the sentence of column Outcomes and Synopsis \n",
    "for x in range(data.Outcomes.count()):\n",
    "    for y in range (len(data.Outcomes[x])):\n",
    "        data.Outcomes[x][y] = data.Outcomes[x][y][0].lower() + data.Outcomes[x][y][1:]\n",
    "for x in range(data.Synopsis.count()):\n",
    "    for y in range (len(data.Synopsis[x])):\n",
    "        data.Synopsis[x][y] = data.Synopsis[x][y][0].lower() + data.Synopsis[x][y][1:]\n",
    "        \n",
    "data.Outcomes[0]\n",
    "#data.Synopsis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Tokenize word by using the regular expression \"\\w+(?:[-']\\w+)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Merge the content of column Synopsis and Outcomes into a new column Content\n",
    "data[\"Content\"] = data[[\"Synopsis\", \"Outcomes\"]].apply(lambda x: x[0] + x[1], axis=1)\n",
    "\n",
    "# Import tokenize module\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "# Create a function that breaks a long sequence of characters into word tokens\n",
    "def tokenizer(sentence):\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")\n",
    "    unigram_tokens = tokenizer.tokenize(sentence)\n",
    "    return unigram_tokens\n",
    "\n",
    "#Apply the function above into the content of column Content\n",
    "data['Content_tokens'] = data.Content.apply(lambda x: [z for y in x for z in tokenizer(y)])\n",
    "\n",
    "#data.Content_tokens[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Return a set of vocabulary with first 200 meaningfull bigrams included in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append all unigrams that extracted from step 5 into an empty list\n",
    "i = 0\n",
    "all_tokens = []\n",
    "while i < data.Content_tokens.count():\n",
    "    all_tokens += data.Content_tokens[i]\n",
    "    i += 1\n",
    "    \n",
    "#Find first 200 meaningfull bigrams from all tokens\n",
    "#Import tools to identify collocations and provide functionalities, dependent on being provided a \n",
    "#function which scores angram given appropriate frequency counts\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "#Find bigram collocations from all tokens\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "#Take first 200 bigrams \n",
    "bigrams = finder.nbest(BigramAssocMeasures.pmi, 200)\n",
    "\n",
    "#Tokenize with 200 bigrams extracted above \n",
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_tokenizer = MWETokenizer(bigrams)\n",
    "all_tokens = mwe_tokenizer.tokenize(all_tokens)\n",
    "\n",
    "#Retokenize the column Cotent_tokens \n",
    "tokenize_content_tokens = []\n",
    "for each in data.Content_tokens:\n",
    "    each = mwe_tokenizer.tokenize(each)\n",
    "    tokenize_content_tokens.append(each)\n",
    "    \n",
    "for i in range(len(tokenize_content_tokens)):\n",
    "    data.Content_tokens[i] = tokenize_content_tokens[i]\n",
    "\n",
    "#Convert the list of all tokens into a set to remove tokens which are the same with others\n",
    "vocab = set(all_tokens)\n",
    "\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Remove the context-independent and context-dependent stopwords from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, remove stopwords from the vocabulary\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "vocab = vocab - stopwords\n",
    "\n",
    "# Second, remove context-dependent from the vocabulary\n",
    "\n",
    "# Import module that helps identify common words\n",
    "from nltk.probability import *\n",
    "from itertools import chain\n",
    "all_tokens2 = list(chain.from_iterable([set(token) for token in data.Content_tokens]))\n",
    "# Compute this distribution from a set of word tokens all_tokens2\n",
    "fd_2 = FreqDist(all_tokens2)\n",
    "# fd_2.most_common()\n",
    "\n",
    "# Remove words with the threshold set to %95 from the vocabulary\n",
    "frequent_words = set()\n",
    "for value in fd_2.most_common():\n",
    "    if value[1] > 0.95 * data.Title.count():\n",
    "        frequent_words.add(value[0])\n",
    "# print(frequent_words)\n",
    "vocab = vocab - frequent_words\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Remove rare tokens from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove words with the threehold set to %5 from the vocabulary\n",
    "rare_words = set()\n",
    "for value in fd_2.most_common():\n",
    "    if value[1] < 0.05 * data.Title.count():\n",
    "        rare_words.add(value[0])\n",
    "vocab = {w for w in vocab if w not in rare_words}\n",
    "#vocab = list(vocab)\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Stem tokens by using Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "vocab_stemmer = set()\n",
    "for each in vocab:\n",
    "    vocab_stemmer.add(ps.stem(each))\n",
    "# vocab_stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Remove tokens with the length less than 3 from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_three_words = set()\n",
    "for each in vocab_stemmer:\n",
    "    if len(each) < 3:\n",
    "        set_three_words.add(each)\n",
    "vocab_stemmer = vocab_stemmer - set_three_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11: Extract vocabulary into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary with for each word in the list vocab_stemmer above\n",
    "inverse_vocab = {}\n",
    "for i, each in enumerate(sorted(vocab_stemmer)):\n",
    "    inverse_vocab[each] = i\n",
    "    \n",
    "with open('29911508_vocab.txt', 'w') as f:\n",
    "    for k, v in inverse_vocab.items():\n",
    "        f.write(f'{k}:{v}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12: Create a function that counts vector of tokens and extracts data into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem tokens of each unit of the column Content_tokens\n",
    "ps = PorterStemmer()\n",
    "data['Content_tokens_stem'] = data['Content_tokens'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "#Create a new column Vector to count vector of each token\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=inverse_vocab, lowercase=False, preprocessor=lambda x: x,\n",
    "                            tokenizer=lambda x: x)\n",
    "data['Vector'] = data['Content_tokens_stem'].apply(lambda x: vectorizer.fit_transform([x]))\n",
    "\n",
    "def write_countvec(file, content):\n",
    "    title = content[0]\n",
    "    file.write(f'{title}, ')\n",
    "    \n",
    "    vector = content[1]\n",
    "    vector_str = ','.join([f'{i}:{c}' for i, c in zip(vector.indices, vector.data)])\n",
    "    file.write(vector_str)\n",
    "    file.write('\\n')\n",
    "        \n",
    "with open('countVec.txt', 'w') as f:\n",
    "    data[['Title', 'Vector']].apply(lambda x: write_countvec(f, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}